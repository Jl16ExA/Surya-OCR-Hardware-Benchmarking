{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jl16ExA/Surya-OCR-Hardware-Benchmarking/blob/main/Surya_OCR_Benchmarking_Across_Different_GPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek9iPQ_GrWk1"
      },
      "source": [
        "# Surya-OCR-Benchmarking Across Different GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is dedicated to benchmarking the performance of the Surya OCR system across various hardware setups, focusing on GPU utilization and scalability.\n",
        "\n",
        "This is still an early version I made in a Friday evening only testing a single image being processed in different batch sizes.\n",
        "\n",
        "If you have any questions or wish to collaborate, feel free to reach out via my social media platforms. I am always eager to discuss and help.\n",
        "\n",
        "- Developer: Juan David López\n",
        "- GitHub: [Jl16ExA](https://github.com/Jl16ExA/find_your_class_javeriana)\n",
        "- LinkedIn:  [Juan David López Becerra](https://www.linkedin.com/in/juan-david-lopez-becerra-5048271bb/)\n",
        "- Twitter:[ @JLopez_160](https://twitter.com/JLopez_160)"
      ],
      "metadata": {
        "id": "wFqELkFlrYEH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKXObERErWk3"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1dR1dcDrWk3",
        "outputId": "3a9fb6c2-4c4f-456d-e3f4-b1bd20563081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Requirement already satisfied: GPUtil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install py-cpuinfo\n",
        "!pip install GPUtil\n",
        "!pip install surya-ocr\n",
        "!pip install pdf2image Pillow reportlab\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEPdz17mrWk4"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaiQsicErWk5"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from surya.ocr import run_ocr\n",
        "from surya.model.detection.segformer import load_model as load_det_model, load_processor as load_det_processor\n",
        "from surya.model.recognition.model import load_model as load_rec_model\n",
        "from surya.model.recognition.processor import load_processor as load_rec_processor\n",
        "\n",
        "langs = [\"es\"] # Replace with your languages\n",
        "det_processor, det_model = load_det_processor(), load_det_model()\n",
        "rec_model, rec_processor = load_rec_model(), load_rec_processor()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Benchmark"
      ],
      "metadata": {
        "id": "5iDrSqmltK_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBUWj7ozrWk5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import platform\n",
        "import cpuinfo\n",
        "import GPUtil\n",
        "import psutil\n",
        "\n",
        "# Define standard deep learning batch sizes to test\n",
        "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "results = pd.DataFrame()\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = \"path/to/png\"\n",
        "image = Image.open(image_path)\n",
        "image = image.resize((2048, int(image.height * 2048 / image.width)))\n",
        "\n",
        "def get_system_resources():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        gpu_name = gpu.name\n",
        "        total_gpu_memory = gpu.memoryTotal\n",
        "        free_gpu_memory = gpu.memoryFree\n",
        "        used_gpu_memory = gpu.memoryUsed\n",
        "    else:\n",
        "        gpu_name = 'No GPU detected'\n",
        "        total_gpu_memory = free_gpu_memory = used_gpu_memory = 0\n",
        "\n",
        "    virtual_memory = psutil.virtual_memory()\n",
        "    total_ram = virtual_memory.total / (1024 ** 3)  # Convert from Bytes to GB\n",
        "    available_ram = virtual_memory.available / (1024 ** 3)\n",
        "    used_ram = virtual_memory.used / (1024 ** 3)\n",
        "\n",
        "    return {\n",
        "        'CPU': cpuinfo.get_cpu_info()['brand_raw'],\n",
        "        'OS': platform.system() + \" \" + platform.release(),\n",
        "        'GPU': gpu_name,\n",
        "        'Total GPU Memory (GB)': total_gpu_memory / 1024,  # Convert MB to GB\n",
        "        'Free GPU Memory (GB)': free_gpu_memory / 1024,\n",
        "        'Used GPU Memory (GB)': used_gpu_memory / 1024,\n",
        "        'Total RAM (GB)': total_ram,\n",
        "        'Available RAM (GB)': available_ram,\n",
        "        'Used RAM (GB)': used_ram\n",
        "    }\n",
        "\n",
        "memory_error_occurred = False\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    os.environ['RECOGNITION_BATCH_SIZE'] = str(batch_size)\n",
        "    images = [image.copy() for _ in range(batch_size)]\n",
        "    print(f\"Running batch size: {batch_size}\")\n",
        "    if not memory_error_occurred:\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            predictions_ocr = run_ocr(images, [langs]*batch_size, det_model, det_processor, rec_model, rec_processor)  # Define these variables and function\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(f\"Batch Size {batch_size}: Completed in {elapsed_time:.2f} seconds.\")\n",
        "        except RuntimeError as e:\n",
        "            if \"CUDA out of memory\" in str(e):\n",
        "                elapsed_time = None\n",
        "                memory_error_occurred = True\n",
        "                print(f\"Batch Size {batch_size} failed: CUDA out of memory.\")\n",
        "            else:\n",
        "                raise  # Re-raise the exception if it's not a memory error\n",
        "    else:\n",
        "        elapsed_time = None\n",
        "        print(f\"Skipping Batch Size {batch_size} due to previous CUDA out of memory error.\")\n",
        "\n",
        "    system_resources = get_system_resources()\n",
        "    system_resources.update({\n",
        "        'Batch Size': batch_size,\n",
        "        'Elapsed Time': elapsed_time,\n",
        "        'Error': 'CUDA out of memory' if elapsed_time is None else None\n",
        "    })\n",
        "    new_row = pd.DataFrame([system_resources])\n",
        "    results = pd.concat([results, new_row], ignore_index=True)\n",
        "    results.to_csv('ocr_benchmark_results.csv', index=False)\n",
        "\n",
        "print(\"Benchmarking completed. Results saved to 'ocr_benchmark_results.csv'.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}